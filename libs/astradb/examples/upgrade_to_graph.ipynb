{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with AstraDB Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/Users/eric.pinzur/src/github.com/langchain-ai/langchain-datastax/libs/astradb/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from langchain_core.documents import Document\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ASTRA_DB_API_ENDPOINT\"] = getpass.getpass(\"ASTRA_DB_API_ENDPOINT = \")\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass.getpass(\"ASTRA_DB_APPLICATION_TOKEN = \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_chunk(chunk: Document, header=None):\n",
    "    if header is None:\n",
    "        header = \"Chunk id, metadata and text\"\n",
    "    print(f\"{header}:\\n\\n'{chunk.id}'\\n\\n{json.dumps(chunk.metadata, indent=4)}\\n\\n\\{chunk.page_content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"header_1\"),\n",
    "    (\"##\", \"header_2\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    return_each_line=False,\n",
    "    strip_headers=False\n",
    ")\n",
    "\n",
    "file_paths = sorted(glob(pathname=\"datasets/legal/*.md\"))\n",
    "\n",
    "all_chunks = []\n",
    "for file_index, file_path in enumerate(file_paths):\n",
    "    with open(file_path, 'r') as file:\n",
    "        chunks = markdown_splitter.split_text(file.read())\n",
    "\n",
    "        for chunk_index, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"file_path\"] = file_path\n",
    "            chunk.id = f\"file_{file_index}_chunk_{chunk_index}\"\n",
    "\n",
    "            all_chunks.append(chunk)\n",
    "\n",
    "print(f\"Split the {len(file_paths)} files into {len(all_chunks)} chunks.\\n\")\n",
    "debug_chunk(all_chunks[3], header=\"Example chunk id, metadata, and content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = AstraDBVectorStore(\n",
    "    collection_name=\"astra_graph_upgrade\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    ")\n",
    "\n",
    "added_ids = vector_store.add_documents(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "What are the consequences if the Developer for\n",
    "the AI-Powered Customer Support Tool fails to\n",
    "meet the Phase 2 delivery date?\n",
    "\"\"\"\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "    },\n",
    ")\n",
    "retrieved_chunks1 = retriever.invoke(query)\n",
    "for chunk in retrieved_chunks1:\n",
    "    debug_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in retrieved_chunks1:\n",
    "    print(chunk.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upgrade to AstraDB Graph Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_astradb import AstraDBGraphVectorStore\n",
    "\n",
    "import re\n",
    "from langchain_community.graph_vectorstores.links import Link\n",
    "from langchain_community.graph_vectorstores.extractors import KeybertLinkExtractor\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_store = AstraDBGraphVectorStore(\n",
    "    collection_name=\"astra_graph_upgrade\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_retriever = graph_vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "    },\n",
    ")\n",
    "retrieved_chunks2 = graph_retriever.invoke(query)\n",
    "for chunk in retrieved_chunks2:\n",
    "    debug_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk1, chunk2 in zip(retrieved_chunks1, retrieved_chunks2):\n",
    "    print(f\"{chunk1.id}\\t{chunk2.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex pattern\n",
    "outgoing_section_pattern = r\"(\\d+\\.\\d+)\\s+\\*\\*(.*?)\\*\\*\"\n",
    "incoming_internal_section_pattern = r\"\\*\\*Section\\s(\\d+\\.\\d+)\\*\\*\"\n",
    "incoming_external_section_pattern1 = r\"\\*\\*(.*?)\\s\\((.*?)\\),\\sSection\\s(\\d+\\.\\d+)\\*\\*\"\n",
    "incoming_external_section_pattern2 = r\"\\*\\*Section\\s(\\d+\\.\\d+)\\sof\\sthe\\s(.*?)\\s\\((.*?)\\)\\*\\*\"\n",
    "\n",
    "keybert_link_extractor = KeybertLinkExtractor(\n",
    "    extract_keywords_kwargs={\n",
    "        \"vectorizer\": KeyphraseCountVectorizer(stop_words=\"english\"),\n",
    "        \"use_mmr\":True,\n",
    "        \"diversity\": 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "def get_links_for_chunk(chunk: Document) -> set[Link]:\n",
    "    doc_title = chunk.metadata.get(\"header_1\", \"\")\n",
    "\n",
    "    links = keybert_link_extractor.extract_one(chunk)\n",
    "\n",
    "    # find outgoing links\n",
    "    for out_section in re.findall(outgoing_section_pattern, chunk.page_content):\n",
    "        out_number = out_section[0]\n",
    "        links.add(Link(\"section\", direction=\"in\", tag=f\"{doc_title} {out_number}\"))\n",
    "\n",
    "    # find incoming links\n",
    "    for in_number in re.findall(incoming_internal_section_pattern, chunk.page_content):\n",
    "        links.add(Link(\"section\", direction=\"out\", tag=f\"{doc_title} {in_number}\"))\n",
    "\n",
    "    for in_section1 in re.findall(incoming_external_section_pattern1, chunk.page_content):\n",
    "        in_title1 = in_section1[0]\n",
    "        in_abbreviation1 = in_section1[1]\n",
    "        in_number1 = in_section1[2]\n",
    "        links.add(Link(\"section\", direction=\"out\", tag=f\"{in_title1} ({in_abbreviation1}) {in_number1}\"))\n",
    "\n",
    "    for in_section2 in re.findall(incoming_external_section_pattern2, chunk.page_content):\n",
    "        in_number2 = in_section2[0]\n",
    "        in_title2 = in_section2[1]\n",
    "        in_abbreviation2 = in_section2[2]\n",
    "        links.add(Link(\"section\", direction=\"out\", tag=f\"{in_title2} ({in_abbreviation2}) {in_number2}\"))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    updated_chunks = graph_vector_store.upgrade_chunks(link_function=get_links_for_chunk)\n",
    "    if updated_chunks == 0:\n",
    "        break\n",
    "\n",
    "    print(f\"Added links to {updated_chunks} chunks.\")\n",
    "\n",
    "print(\"Upgrade Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_retriever = graph_vector_store.as_retriever(\n",
    "    search_type=\"mmr_traversal\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"fetch_k\": 20, # initial starting chunks\n",
    "        \"depth\": 2,\n",
    "    },\n",
    ")\n",
    "retrieved_chunks3 = graph_retriever.invoke(query)\n",
    "for chunk in retrieved_chunks3:\n",
    "    debug_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk1, chunk2, chunk3 in zip(retrieved_chunks1, retrieved_chunks2, retrieved_chunks3):\n",
    "    print(f\"{chunk1.id}\\t{chunk2.id}\\t{chunk3.id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
